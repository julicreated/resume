#..........HPC1.....................#
#include<iostream>
#include<stdlib.h>
#include<queue>
using namespace std;
class node
{
 public:
 
 node *left, *right;
 int data;
}; 
class Breadthfs
{
public:
node *insert(node *, int);
void bfs(node *);
};
node *insert(node *root, int data)
// inserts a node in tree
{
 if(!root)
 {
 
 root=new node;
 root->left=NULL;
 root->right=NULL;
 root->data=data;
 return root;
 }
 queue<node *> q;
 q.push(root);
 
 while(!q.empty())
 {
 node *temp=q.front();
 q.pop();
 
 if(temp->left==NULL)
 {
 
 temp->left=new node;
 temp->left->left=NULL;
 temp->left->right=NULL;
 temp->left->data=data; 
 return root;
 }
 else
 {
 q.push(temp->left);
 }
 if(temp->right==NULL)
 {
 
 temp->right=new node;
 temp->right->left=NULL;
 temp->right->right=NULL;
 temp->right->data=data; 
 return root;
 }
 else
 {
 q.push(temp->right);
 }
 }
 
}
void bfs(node *head)
{
 queue<node*> q;
 q.push(head);
 
 int qSize;
 
 while (!q.empty())
 {
 qSize = q.size();
 #pragma omp parallel for
 //creates parallel threads
 for (int i = 0; i < qSize; i++)
 {
 node* currNode;
 #pragma omp critical
 {
 currNode = q.front();
 q.pop();
 cout<<"\t"<<currNode->data;
 
 }// prints parent node
 #pragma omp critical
 {
 if(currNode->left)// push parent's left node in queue
 q.push(currNode->left);
 if(currNode->right)
 q.push(currNode->right);
 }// push parent's right node in queue 
 }
 }
}
int main(){
 node *root=NULL;
 int data;
 char ans;
 
 do
 {
 cout<<"\n enter data=>";
 cin>>data;
 
 root=insert(root,data);
 
 cout<<"do you want insert one more node?";
 cin>>ans;
 
 }while(ans=='y'||ans=='Y');
 
 bfs(root);
 
 return 0;
}

#............................HPC1B.............#

#include <iostream>
#include <vector>
#include <stack>
#include <omp.h>
using namespace std;
const int MAX = 100000;
vector<int> graph[MAX];
bool visited[MAX];
void dfs(int node) {
stack<int> s;
s.push(node);
while (!s.empty()) {
 int curr_node = s.top();
 s.pop();
 if (!visited[curr_node]) {
 visited[curr_node] = true;
 
 if (visited[curr_node]) {
 cout << curr_node << " ";
 }
 #pragma omp parallel for
 for (int i = 0; i < graph[curr_node].size(); i++) {
 int adj_node = graph[curr_node][i];
 if (!visited[adj_node]) {
 s.push(adj_node);
 }
 }
 }
}
}
int main() {
int n, m, start_node;
cout << "Enter No of Node,Edges,and start node:" ;
cin >> n >> m >> start_node;
 //n: node,m:edges
 
cout << "Enter Pair of edges:" ;
for (int i = 0; i < m; i++) {
 int u, v;
 
 cin >> u >> v;
//u and v: Pair of edges
 graph[u].push_back(v);
 graph[v].push_back(u);
}
#pragma omp parallel for
for (int i = 0; i < n; i++) {
 visited[i] = false;
}
dfs(start_node);
/* for (int i = 0; i < n; i++) {
 if (visited[i]) {
 cout << i << " ";
 }
}*/
return 0;
}

#...........................HPC2A............#

#include<iostream>
#include<stdlib.h>
#include<omp.h>
using namespace std;
void bubble(int *, int);
void swap(int &, int &);
void bubble(int *a, int n)
{
 for( int i = 0; i < n; i++ )
 { 
 int first = i % 2; 
 #pragma omp parallel for shared(a,first)
 for( int j = first; j < n-1; j += 2 )
 { 
 if( a[ j ] > a[ j+1 ] )
 { 
 swap( a[ j ], a[ j+1 ] );
 } 
 } 
 }
}
void swap(int &a, int &b)
{
 int test;
 test=a;
 a=b;
 b=test;
}
int main()
{
 int *a,n;
 cout<<"\n enter total no of elements=>";
 cin>>n;
 a=new int[n];
 cout<<"\n enter elements=>";
 for(int i=0;i<n;i++)
 {
 cin>>a[i];
 }
 
 bubble(a,n);
 
 cout<<"\n sorted array is=>";
 for(int i=0;i<n;i++)
 {
 cout<<a[i]<<endl;
 }
return 0;
}

#.....................HPC2B.............#

#include<iostream>
#include<stdlib.h>
#include<omp.h>
using namespace std;
void mergesort(int a[],int i,int j);
void merge(int a[],int i1,int j1,int i2,int j2);
void mergesort(int a[],int i,int j)
{
int mid;
if(i<j)
{
 mid=(i+j)/2;
 
 #pragma omp parallel sections
 {
 #pragma omp section
 {
 mergesort(a,i,mid); 
 }
 #pragma omp section
 {
 mergesort(a,mid+1,j); 
 }
 }
 merge(a,i,mid,mid+1,j); 
}
}
void merge(int a[],int i1,int j1,int i2,int j2)
{
int temp[1000]; 
int i,j,k;
i=i1; 
j=i2; 
k=0;
 
while(i<=j1 && j<=j2) 
{
 if(a[i]<a[j])
 {
 temp[k++]=a[i++];
 }
 else
 {
 temp[k++]=a[j++];
 } 
}
 
while(i<=j1) 
{
 temp[k++]=a[i++];
}
 
while(j<=j2) 
{
 temp[k++]=a[j++];
}
 
for(i=i1,j=0;i<=j2;i++,j++)
{
 a[i]=temp[j];
} 
}
int main()
{
int *a,n,i;
cout<<"\n enter total no of elements=>";
cin>>n;
a= new int[n];
cout<<"\n enter elements=>";
for(i=0;i<n;i++)
{
 cin>>a[i];
}
 // start=.......
//#pragma omp‚Ä¶..
mergesort(a, 0, n-1);
// stop‚Ä¶‚Ä¶.
cout<<"\n sorted array is=>";
for(i=0;i<n;i++)
{
 cout<<"\n"<<a[i];
}
 // Cout<<Stop-Start
return 0;
}

#........................HPC3.............#

#include <iostream>
//#include <vector>
#include <omp.h>
#include <climits>
using namespace std;
void min_reduction(int arr[], int n) {
 int min_value = INT_MAX;
 #pragma omp parallel for reduction(min: min_value)
 for (int i = 0; i < n; i++) {
if (arr[i] < min_value) {
 min_value = arr[i];
}
 }
 cout << "Minimum value: " << min_value << endl;
}
void max_reduction(int arr[], int n) {
 int max_value = INT_MIN;
 #pragma omp parallel for reduction(max: max_value)
 for (int i = 0; i < n; i++) {
if (arr[i] > max_value) {
 max_value = arr[i];
}
 }
 cout << "Maximum value: " << max_value << endl;
}
void sum_reduction(int arr[], int n) {
 int sum = 0;
 #pragma omp parallel for reduction(+: sum)
 for (int i = 0; i < n; i++) {
sum += arr[i];
 }
 cout << "Sum: " << sum << endl;
}
void average_reduction(int arr[], int n) {
 int sum = 0;
 #pragma omp parallel for reduction(+: sum)
 for (int i = 0; i < n; i++) {
sum += arr[i];
 }
 cout << "Average: " << (double)sum / (n-1) << endl;
}
int main() {
 int *arr,n;
 cout<<"\n enter total no of elements=>";
 cin>>n;
 arr=new int[n];
 cout<<"\n enter elements=>";
 for(int i=0;i<n;i++)
 {
 cin>>arr[i];
 }
// int arr[] = {5, 2, 9, 1, 7, 6, 8, 3, 4};
// int n = size(arr);
 min_reduction(arr, n);
 max_reduction(arr, n);
 sum_reduction(arr, n);
 average_reduction(arr, n);
}

#..................HPC4A............#
1. !pip install git+https://github.com/andreinechaev/nvcc4jupyter.git
2. !nvcc --version
3. pip install nvcc4jupyter
4. %load_ext nvcc4jupyter

5. %%cuda

#include <stdio.h>

#define HANDLE_ERROR( err ) ( HandleError( err, __FILE__, __LINE__ ) )

static void HandleError( cudaError_t err, const char *file, int line )
{
    if (err != cudaSuccess)
      {
        printf( "%s in %s at line %d\n", cudaGetErrorString( err ),
                file, line );
        exit( EXIT_FAILURE );
    }
}

const short N = 500 ;

__global__ void Vector_Addition ( const int *dev_a , const int *dev_b , int *dev_c)
{
      unsigned short tid = threadIdx.x ;

      if ( tid < N )
            dev_c [tid] = dev_a[tid] + dev_b[tid] ;

}


int main (void)
{
      int Host_a[N], Host_b[N], Host_c[N];


      int *dev_a , *dev_b, *dev_c ;


      HANDLE_ERROR ( cudaMalloc((void **)&dev_a , N*sizeof(int) ) );
      HANDLE_ERROR ( cudaMalloc((void **)&dev_b , N*sizeof(int) ) );
      HANDLE_ERROR ( cudaMalloc((void **)&dev_c , N*sizeof(int) ) );


      for ( int i = 0; i <N ; i++ )
      {
            Host_a[i] = i ;
            Host_b[i] = i*i ;
      }

      HANDLE_ERROR (cudaMemcpy (dev_a , Host_a , N*sizeof(int) , cudaMemcpyHostToDevice));
      HANDLE_ERROR (cudaMemcpy (dev_b , Host_b , N*sizeof(int) , cudaMemcpyHostToDevice));

      Vector_Addition <<< 1, N  >>> (dev_a , dev_b , dev_c ) ;

      HANDLE_ERROR (cudaMemcpy(Host_c , dev_c , N*sizeof(int) , cudaMemcpyDeviceToHost));

      for ( int i = 0; i<N; i++ )
                  printf ("%d + %d = %d\n", Host_a[i] , Host_b[i] , Host_c[i] ) ;

      cudaFree (dev_a) ;
      cudaFree (dev_b) ;
      cudaFree (dev_c) ;

      system("pause");
      return 0 ;

}

#...........................HPC4B......................#
1. !pip install git+https://github.com/andreinechaev/nvcc4jupyter.git
2. !nvcc --version
3. pip install nvcc4jupyter
4. %load_ext nvcc4jupyter
5. %%cuda
#include <stdio.h>

#define TILE_WIDTH 10

/*matrix multiplication kernels*/

//non shared
__global__ void MatrixMul( int *Md , int *Nd , int *Pd , const int WIDTH )
{

           // calculate thread id

           unsigned int col = TILE_WIDTH * blockIdx.x + threadIdx.x ;

           unsigned int row = TILE_WIDTH*blockIdx.y + threadIdx.y ;

           Pd[row*WIDTH + col]=0;

         for (int k = 0 ; k<WIDTH ; k++ )
         {
                  Pd[row*WIDTH + col]+= Md[row * WIDTH + k ] * Nd[ k * WIDTH + col] ;
          }

}


// main routine
int main ()
{
   const int WIDTH = 100 ;      // Change the width to change input size
   float naive_gpu_elapsed_time_ms;
   int array1_h[WIDTH][WIDTH] ,array2_h[WIDTH][WIDTH],result_array_h[WIDTH][WIDTH];
  int *array1_d , *array2_d ,*result_array_d ; // device array
  int i , j ;
  //input in host array
 // printf("Enter matrix1\n");
  for ( i = 0 ; i < WIDTH ; i++ )
  {
     for (j = 0 ; j < WIDTH ; j++ )
     {

    	 array1_h[i][j]=1;
    	 array2_h[i][j]=1;
     }
  }

cudaEvent_t start, stop;

cudaEventCreate(&start);

cudaEventCreate(&stop);

  //create device array cudaMalloc ( (void **)&array_name, sizeofmatrixinbytes) ;

  cudaMalloc((void **) &array1_d , WIDTH*WIDTH*sizeof (int) ) ;

  cudaMalloc((void **) &array2_d , WIDTH*WIDTH*sizeof (int) ) ;




  //copy host array to device array; cudaMemcpy ( dest , source , WIDTH , direction )

  cudaMemcpy ( array1_d , array1_h , WIDTH*WIDTH*sizeof (int) , cudaMemcpyHostToDevice ) ;

  cudaMemcpy ( array2_d , array2_h , WIDTH*WIDTH*sizeof (int) , cudaMemcpyHostToDevice ) ;



  //allocating memory for resultant device array

  cudaMalloc((void **) &result_array_d , WIDTH*WIDTH*sizeof (int) ) ;





  //calling kernal

  dim3 dimGrid ( WIDTH/TILE_WIDTH , WIDTH/TILE_WIDTH ,1 ) ;

  dim3 dimBlock( TILE_WIDTH, TILE_WIDTH, 1 ) ;

cudaEventRecord(start, 0);

  MatrixMul <<<dimGrid,dimBlock>>> ( array1_d , array2_d ,result_array_d , WIDTH) ;

cudaEventRecord(stop, 0);

  cudaMemcpy(result_array_h , result_array_d , WIDTH*WIDTH*sizeof(int) ,cudaMemcpyDeviceToHost) ;



  printf("matrix result\n");
  for ( i = 0 ; i < WIDTH ; i++ )
  {
      for ( j = 0 ; j < WIDTH ; j++ )
     {
        printf ("%d   ",result_array_h[i][j] ) ;
     }
 printf ("\n") ;
}
   cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);
   printf("Time elapsed on naive GPU matrix multiplication of %dx%d . %dx%d : %f ms.\n\n", WIDTH, WIDTH, WIDTH,WIDTH, naive_gpu_elapsed_time_ms);
 system("pause") ;
}

#...................DL1.....................#

1. import tensorflow as tf
   from tensorflow.keras.datasets import boston_housing
   from sklearn import preprocessing

2. (train_x,train_y),(test_x,test_y) = boston_housing.load_data()

3. print("train shape :",train_x.shape)
   print("Test shape :",test_x.shape)
   print("Actual Train Output",train_y.shape)
   print("Actual Test Output",test_y.shape)

4. train_x

5. train_x[0]
6. test_x[0]
7. train_y[0]
8. train_x=preprocessing.normalize(train_x)
   test_x=preprocessing.normalize(test_x)
9. train_x[0]
10. train_y[0]
11. from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import *
    def HousePricePredictionModel():
    model=Sequential()
    model.add(Dense(128,activation='relu',input_shape=(train_x[0].shape)))
    model.add(Dense(64,activation='relu'))
    model.add(Dense(32,activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])
    return model
12. import numpy as np
   k=4
   num_val_samples= len(train_x)
   num_epochs = 100
   all_scores = []
13. model = HousePricePredictionModel()
    history=model.fit(x=train_x,y=train_y,epochs= num_epochs,batch_size=1,verbose=1,validation_data=(test_x,test_y))
14. test = [[0.02675675, 0.        , 0.02677953, 0.        , 0.0010046 ,
            0.00951931, 0.14795322, 0.0027145 , 0.03550877, 0.98536841,
            0.02988655, 0.04031725, 0.04298041]]
    print("Actual Output: ", train_y[0])
    print("Predicted Output: ", model.predict(test))

#...........................DL2..................#

1.from keras.datasets import imdb
  (train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=10000)

2.import numpy as np
  def vectorize_sequences(sequences,dimension=10000) :
     results = np.zeros((len(sequences),dimension))
     for i , sequences in enumerate(sequences):
         results[i,sequences] = 1
         return results


  x_train=vectorize_sequences(train_data)
  x_test=vectorize_sequences(test_data)

  y_train = np.asarray(train_labels).astype('float32')
  y_test = np.asarray(test_labels).astype('float32')

3. from keras import models
   from keras import layers
   model = models.Sequential()
   model.add(layers.Dense(16, activation = 'relu', input_shape=(10000 ,)))
   model.add(layers.Dense(16, activation = 'relu'))
   model.add(layers.Dense(1, activation = 'sigmoid'))

4. model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])
   x_val = x_train[:10000]
   partial_x_train = x_train[10000:]
   y_val = y_train[:10000]
   partial_y_train = y_train[10000:]

   model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])

   history = model.fit(partial_x_train , partial_y_train , epochs=20 , batch_size=512 , validation_data=(x_val,y_val))

5. import matplotlib.pyplot as plt
   history_dict = history.history
   loss_values = history_dict['loss']
   val_loss_values = history_dict['val_loss']
   epochs = range(1,len(loss_values)  + 1)
   plt.plot(epochs,loss_values,'bo', label='Training Loss')
   plt.plot(epochs,val_loss_values,'b', label='Validation Loss')
   plt.title('Training and Validation Loss')
   plt.xlabel('Epochs')
   plt.ylabel('Accuracy')
   plt.legend()
   plt.show()

6.plt.clf()
  acc_values = history_dict['acc']
  val_acc_values = history_dict['val_acc']
  plt.plot(epochs,acc_values,'bo', label='Training Accuracy')
  plt.plot(epochs,val_acc_values,'b', label='Validation Accuracy')
  plt.title('Training and Validation Accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.show()
7.
model.predict(x_test)

#..................DL3..............#

1.
import tensorflow as tf

import numpy as np
import matplotlib.pyplot as plt

# Load the fashion-mnist pre-shuffled train data and test data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)

2.
# Print training set shape - note there are 60,000 training data of image size of 28x28, 60,000 train labels)
print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)

# Print the number of training and test datasets
print(x_train.shape[0], 'train set')
print(x_test.shape[0], 'test set')

# Define the text labels
fashion_mnist_labels = ["T-shirt/top",  # index 0
                        "Trouser",      # index 1
                        "Pullover",     # index 2
                        "Dress",        # index 3
                        "Coat",         # index 4
                        "Sandal",       # index 5
                        "Shirt",        # index 6
                        "Sneaker",      # index 7
                        "Bag",          # index 8
                        "Ankle boot"]   # index 9

# Image index, you can pick any number between 0 and 59,999
img_index = 5
# y_train contains the lables, ranging from 0 to 9
label_index = y_train[img_index]
# Print the label, for example 2 Pullover
print ("y = " + str(label_index) + " " +(fashion_mnist_labels[label_index]))
# # Show one of the images from the training dataset
plt.imshow(x_train[img_index])

3.
w, h = 28, 28
x_train = x_train.reshape(x_train.shape[0], w, h, 1)
x_test = x_test.reshape(x_test.shape[0], w, h, 1)

x_train.shape

4.
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten

model = tf.keras.Sequential()

model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.3))

model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.3))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

model.summary()

5.
BATCH_SIZE=1000
EPOCHS = 20

model.compile(loss='sparse_categorical_crossentropy',
             optimizer='adam',
             metrics=['accuracy'])
%time history = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2, verbose=1)

6.
train_loss, train_accuracy = model.evaluate(x_train, y_train, batch_size=BATCH_SIZE)
train_accuracy

7.
test_loss, test_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
test_accuracy

8.
y_hat = model.predict(x_test)

# Plot a random sample of 10 test images, their predicted labels and ground truth
figure = plt.figure(figsize=(20, 8))
for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):
    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])
    # Display each image
    ax.imshow(np.squeeze(x_test[index]))
    predict_index = np.argmax(y_hat[index])
    true_index = y_test[index]
    # Set the title for each image
    ax.set_title("{} ({})".format(fashion_mnist_labels[predict_index],
                                  fashion_mnist_labels[true_index]),
                                  color=("green" if predict_index == true_index else "red"))
